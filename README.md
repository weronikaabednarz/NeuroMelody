# Analysis of the possibility of using recurrent neural networks to generate new melodies based on the MIDI files

**Summary**

The goal of the thesis project was to analyze the possibilities of using recurrent neural networks to generate melodies, 
taking into account various quality assessment metrics. The research was carried out on a MIDI-format music dataset, 
which was processed and properly prepared for model operation. The network architecture was designed using LSTM network 
layers and regularisation methods. The network model was modified to take into account different parameter configurations 
and to analyse the sequences they generated. The models were evaluated using metrics such as Cross-Entropy Loss, Perplexity 
and Cosine Similarity, and the melodies using metrics such as Pitch Range, Tone Diversity, Repetition Ratio, Melodic Coherence, 
Harmonic Consistency and Melodic Movement Score. The results were subjected to detailed analysis. The study showed that the 
sequence generated by the model with the best-performing metrics had high tonal agreement and fluidity of melodic motion. 
However, some parameters need further optimisation to reduce repeatability. 

The results may find applications in music education and automatic music composition. Further development of the project may include extending the dataset and functionality of the model, as well as applying modern techniques such as GAN or GPT models to improve the quality of the generated melodies and eliminate overfitting.


The **dataset** used is from the kaggle platform:
https://www.kaggle.com/datasets/soumikrakshit/classical-music-midi includes works by 19 different composers performed on the piano. 
I chose pieces composed by Frederic Chopin to train the model.

The project was implemented in Python using a long short-term memory network, implemented in the Google Colab environment, with Google Drive connected to store the dataset and generated sequences. **Technologies** used in the project:
- os - operations on files and directories,
- zipfile - handling of .zip files,
- glob - searching for files according to patterns,
- pandas - processing of tabular data,
- numpy - operations on arrays, matrices and vectors,
- time - time measurement,
- math - mathematical functions,
- scipy.stats - calculation of model evaluation metrics,
- matplotlib.pyplot - creating graphs,
- seaborn - data visualisation, supporting matplotlib,
- IPython.display - displaying images and sounds,
- music21 - working with music structures,
- tensorflow.keras - building machine learning models, including Keras, which offers tools such as neural network layers (e.g. LSTM, Dense, Dropout) for creating sequential models, the Adamax optimiser for efficient training, and the to_categorical function for coding categories, all within the TensorFlow framework,
- sklearn - splitting data into training and test sets and evaluating model performance using metrics such as cosine similarity,
- google.colab - handling Google Drive,
- warnings - handling warnings.
   
**Dataset:**

![1](https://github.com/weronikaabednarz/Engineering-Thesis/blob/main/images/used_dataset.jpg)
![2](https://github.com/weronikaabednarz/Engineering-Thesis/blob/main/images/melodies_used.jpg)

**Appearance of the interface for generating new melodies:**

![3](https://github.com/weronikaabednarz/Engineering-Thesis/blob/main/images/interface.jpg)

The interface created allows the user to easily adjust the parameters of the melody, such as the duration of the song (in seconds) using a slider and the tempo selected from a drop-down list (slow, moderate, fast). This solution allows new melodies to be dynamically generated without having to look into the code.

**Model configurations included:**

![4](https://github.com/weronikaabednarz/Engineering-Thesis/blob/main/images/melody_evaluation_metrics.jpg)

**Results for selected model evaluation metrics:**

![5](https://github.com/weronikaabednarz/Engineering-Thesis/blob/main/images/model_evaluation_metrics.jpg)

Analysis of the training times showed a significant difference between the shortest and longest times. The shortest time was 16 minutes (Model 30), while the longest time reached 358 minutes (Model 26). This difference highlights the variation in performance of the models used in the experiments. Training time increases approximately linearly with the number of epochs, which is expected. Models trained for 100 epochs (e.g. Model 14, Model 15) have significantly longer training times compared to those trained for 10 or 50 epochs.

A higher dropout (e.g. 0.5 in Model 15, 17, 27, 40) leads to an increase in the regularity of the model, which helps to avoid overfitting. These models maintain stable performance on the test set, but may have difficulty learning more complex patterns.
For both the training and test set, lower values of the Cross-Entropy Loss function indicate better model fit. Analysing the results obtained, it can be seen that models consisting of two LSTM layers and smaller dropout values achieve the lowest values for this metric - for example, model 14 and model 26. Furthermore, models trained for a higher number of epochs (100) tend to achieve lower Cross Entropy Loss values.

The values of the Cross-Entropy Loss and Perplexity metrics are correlated. Models characterised by a low value of Perplexity for the test set are more predictable and generalise better, which means that the difference between the results for the two sets is small.
When considering the results obtained, it can be seen that the lowest values on the training set for this metric are obtained by models: 13, 14, 18, 26, 36 and 37, but these values for the test set are already much higher for these models, which may indicate a tendency towards overlearning. In contrast, satisfactory Cross-Entropy Loss and Perplexity results for the training and test set were obtained for models: 8, 15, 16, 17, 40. Considering in the evaluation of the models also the Cosine Similarity metric, whose higher values indicate a better representation of the similarity between the model predictions and the actual data, it can be seen that model 8 obtained very poor results.

From the results obtained, it can be concluded that **model number 16** achieves the most satisfactory results of all the models analysed. It is characterised by a relatively low difference in Cross-Entropy Loss and Perplexity values between the training set and the test set, indicating good generalisability. In addition, the very high value of the Cosine Similarity metric suggests that the model is a good fit to the data. It is particularly noteworthy that such good results were obtained with a dropout of 0.3, which helps to reduce the risk of overfitting.

**Results for selected metrics for evaluating the generated melodies:**

![6](https://github.com/weronikaabednarz/Engineering-Thesis/blob/main/images/melody_evaluation_metrics.jpg)

Melodies for which the value of the Pitch Range metric reaches high values (e.g. 50 for model 16) are characterised by a wide range of notes in the generated sequence. In contrast, low scores of the metric (e.g. 1 for model 32) are indicative of a narrow range of notes used to create the song, which can result in monotony. High values of the Tone Diversity metric (e.g. 0.26 for model 37) indicate a greater variety of sounds present. A low Repetition Ratio metric (e.g. 0.0204 for model 3) characterises sequences with few repeated fragments. Low values of the Melodic Coherence metric (e.g. 0.0021 for model 29) suggest irregularity in the melodic structure. On the other hand, high scores obtained for Harmonic Consistency (e.g. 0.96 for model 5) are indicative of the harmonic coherence of a piece in a particular key. A high Melodic Movement Score (e.g. 0.9556 for model 8) indicates fluidity and natural movement of sounds in the melody.
